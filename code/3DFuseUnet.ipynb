{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 2542390,
          "sourceType": "datasetVersion",
          "datasetId": 1541666
        }
      ],
      "dockerImageVersionId": 30615,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* filtre = 8\n",
        "* n_patch = 1\n",
        "* lr = dynamic\n"
      ],
      "metadata": {
        "id": "jeMpLzMHOLCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install elasticdeform\n",
        "! pip install tensorflow_addons\n",
        "! pip install gdown"
      ],
      "metadata": {
        "id": "4jwTkCT08Q13",
        "execution": {
          "iopub.status.busy": "2023-12-08T01:30:16.741544Z",
          "iopub.execute_input": "2023-12-08T01:30:16.742091Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTS\n",
        "import gdown\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tarfile\n",
        "import nibabel as nib\n",
        "import glob\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sys import stdout\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpim\n",
        "from scipy.ndimage.interpolation import affine_transform\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import numpy as np\n",
        "from scipy.ndimage.interpolation import affine_transform\n",
        "import elasticdeform\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nibabel as nib\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Conv3D, Conv3DTranspose, Dropout, ReLU, LeakyReLU, Concatenate, ZeroPadding3D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.layers import InstanceNormalization\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "from sklearn.metrics import precision_score\n",
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "import math\n",
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "IqQEZJMSOLC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORT INPUT FOLDER FROM DRIVE\n",
        "inputs_file = 'https://drive.google.com/file/d/1eSUN5_gtCkpPn277GpLe-Pr_suiYBtWU/view?usp=sharing'\n",
        "inputs_url = 'https://drive.google.com/uc?id=1eSUN5_gtCkpPn277GpLe-Pr_suiYBtWU'\n",
        "\n",
        "inputs_output = 'inputs.zip'\n",
        "gdown.download(inputs_url, inputs_output, quiet=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T04:59:33.523638Z",
          "iopub.execute_input": "2022-05-01T04:59:33.523944Z",
          "iopub.status.idle": "2022-05-01T04:59:34.632995Z",
          "shell.execute_reply.started": "2022-05-01T04:59:33.523902Z",
          "shell.execute_reply": "2022-05-01T04:59:34.632339Z"
        },
        "trusted": true,
        "id": "-V-fBll6OLC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip ./inputs.zip -d ./INPUTS\n",
        "! mv -v ./INPUTS/content/drive/MyDrive/GANs/BrainTumorDetection/INPUTS_2021/* ./INPUTS\n",
        "! rm -rf ./INPUTS/content\n",
        "! rm ./inputs.zip"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T04:59:34.635176Z",
          "iopub.execute_input": "2022-05-01T04:59:34.635607Z",
          "iopub.status.idle": "2022-05-01T04:59:37.408518Z",
          "shell.execute_reply.started": "2022-05-01T04:59:34.635566Z",
          "shell.execute_reply": "2022-05-01T04:59:37.407478Z"
        },
        "trusted": true,
        "id": "qpAEzn0JOLDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Eeach execution has 5 epochs\n",
        "#kaggle can run 12 hours at a time\n",
        "EXEC = 2\n",
        "VERSION = 'fusech3'\n",
        "FUSECHANNEL=3\n",
        "INPUT_PATH = './INPUTS'\n",
        "PATH = './RESULTS'\n",
        "DATA_PATH = './data'"
      ],
      "metadata": {
        "id": "pkjo9C7Ah9bZ",
        "execution": {
          "iopub.status.busy": "2022-05-01T04:59:37.414226Z",
          "iopub.execute_input": "2022-05-01T04:59:37.416169Z",
          "iopub.status.idle": "2022-05-01T04:59:37.422808Z",
          "shell.execute_reply.started": "2022-05-01T04:59:37.416128Z",
          "shell.execute_reply": "2022-05-01T04:59:37.42182Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import result folder  from previous epoch to continue the training\n",
        "if EXEC!=1:\n",
        "\n",
        "    results_file = 'https://drive.google.com/file/d/17ct0iVhvQ7VgL3bEAp-GkTc915j7Rq0i/view?usp=sharing'\n",
        "    results_url = 'https://drive.google.com/uc?id=17ct0iVhvQ7VgL3bEAp-GkTc915j7Rq0i'\n",
        "\n",
        "    results_output = 'results.zip'\n",
        "\n",
        "    gdown.download(results_url, results_output, quiet=False)\n",
        "\n",
        "else:\n",
        "    ! mkdir ./RESULTS"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T04:59:37.427158Z",
          "iopub.execute_input": "2022-05-01T04:59:37.428289Z",
          "iopub.status.idle": "2022-05-01T04:59:38.178132Z",
          "shell.execute_reply.started": "2022-05-01T04:59:37.428247Z",
          "shell.execute_reply": "2022-05-01T04:59:38.177084Z"
        },
        "trusted": true,
        "id": "UmFo92I2OLDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if EXEC!=1:\n",
        "    ! unzip ./results.zip -d ./RESULTS\n",
        "    ! mv -v ./RESULTS/content/drive/MyDrive/GANs/BrainTumorDetection/RESULTS_KAGGLE/VERSION_$VERSION/RESULTS_{EXEC-1}/* ./RESULTS\n",
        "    ! rm -rf ./RESULTS/content\n",
        "    ! rm ./results.zip"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T04:59:38.179715Z",
          "iopub.execute_input": "2022-05-01T04:59:38.179982Z",
          "iopub.status.idle": "2022-05-01T04:59:38.19032Z",
          "shell.execute_reply.started": "2022-05-01T04:59:38.179943Z",
          "shell.execute_reply": "2022-05-01T04:59:38.189257Z"
        },
        "trusted": true,
        "id": "u4U6OGRiOLDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PATCH EXTRACTION AND AUGMENTATION\n",
        "\n",
        "def patch_extraction(Xb, yb, sizePatches=128, Npatches=1, max_tries = 50, background_threshold=0.95, num_classes=4, apply_ratio=False):\n",
        "    \"\"\"\n",
        "    3D patch extraction\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, rows, columns, slices, channels = Xb.shape\n",
        "    X_patches = np.empty((batch_size*Npatches, sizePatches, sizePatches, sizePatches, channels))\n",
        "    y_patches = np.empty((batch_size*Npatches, sizePatches, sizePatches, sizePatches))\n",
        "    i = 0\n",
        "    list_id = []\n",
        "\n",
        "    if apply_ratio:\n",
        "      #print(f'Application of ratio(bg_ratio < {background_threshold*100}%): {apply_ratio}')\n",
        "      for b in range(batch_size):\n",
        "          for p in range(Npatches):\n",
        "\n",
        "              temp_X = None\n",
        "              temp_y = None\n",
        "\n",
        "              tries = 0\n",
        "              out = False\n",
        "              while (tries < max_tries) and (not out):\n",
        "                #choose a random position from which the patch starts\n",
        "                x = np.random.randint(rows-sizePatches+1)\n",
        "                y = np.random.randint(columns-sizePatches+1)\n",
        "                z = np.random.randint(slices-sizePatches+1)\n",
        "\n",
        "                # extract the brain patch\n",
        "\n",
        "                temp_X = Xb[b, x:x+sizePatches, y:y+sizePatches, z:z+sizePatches, :]\n",
        "\n",
        "                # extract GT\n",
        "                temp_y = yb[b, x:x+sizePatches, y:y+sizePatches, z:z+sizePatches]\n",
        "\n",
        "                # One-hot Encoding\n",
        "                temp_y_cat = keras.utils.to_categorical(temp_y, num_classes=num_classes)\n",
        "\n",
        "\n",
        "                bgrd_ratio = np.sum(temp_y_cat[:, :, :, 0])/(sizePatches * sizePatches * sizePatches)\n",
        "\n",
        "                tries += 1\n",
        "\n",
        "                if bgrd_ratio < background_threshold:\n",
        "                  out = True\n",
        "                  temp_X = Xb[b, x:x+sizePatches, y:y+sizePatches, z:z+sizePatches, :]\n",
        "\n",
        "              #print(f'\\nBG_ratio : {bgrd_ratio*100} %')\n",
        "              #print(f\"\\nTried {tries} times to find a sub-volume. Giving up...\")\n",
        "              X_patches[i] = temp_X\n",
        "              y_patches[i] = temp_y\n",
        "              i += 1\n",
        "\n",
        "    else:\n",
        "      for b in range(batch_size):\n",
        "        for p in range(Npatches):\n",
        "            x = np.random.randint(rows-sizePatches+1)\n",
        "            y = np.random.randint(columns-sizePatches+1)\n",
        "            z = np.random.randint(slices-sizePatches+1)\n",
        "\n",
        "            X_patches[i] = Xb[b, x:x+sizePatches, y:y+sizePatches, z:z+sizePatches, :]\n",
        "            y_patches[i] = yb[b, x:x+sizePatches, y:y+sizePatches, z:z+sizePatches]\n",
        "            i += 1\n",
        "\n",
        "    return X_patches, y_patches\n",
        "\n",
        "def flip3D(X, y):\n",
        "    \"\"\"\n",
        "    Flip the 3D image respect one of the 3 axis chosen randomly\n",
        "    \"\"\"\n",
        "    choice = np.random.randint(3)\n",
        "    if choice == 0: # flip on x\n",
        "        X_flip, y_flip = X[::-1, :, :, :], y[::-1, :, :]\n",
        "    if choice == 1: # flip on y\n",
        "        X_flip, y_flip = X[:, ::-1, :, :], y[:, ::-1, :]\n",
        "    if choice == 2: # flip on z\n",
        "        X_flip, y_flip = X[:, :, ::-1, :], y[:, :, ::-1]\n",
        "\n",
        "    return X_flip, y_flip\n",
        "\n",
        "\n",
        "def rotation3D(X, y):\n",
        "    \"\"\"\n",
        "    Rotate a 3D image with alfa, beta and gamma degree respect the axis x, y and z respectively.\n",
        "    The three angles are chosen randomly between 0-30 degrees\n",
        "    \"\"\"\n",
        "    alpha, beta, gamma = np.pi*np.random.random_sample(3,)/6\n",
        "    Rx = np.array([[1, 0, 0],\n",
        "                   [0, np.cos(alpha), -np.sin(alpha)],\n",
        "                   [0, np.sin(alpha), np.cos(alpha)]])\n",
        "\n",
        "    Ry = np.array([[np.cos(beta), 0, np.sin(beta)],\n",
        "                   [0, 1, 0],\n",
        "                   [-np.sin(beta), 0, np.cos(beta)]])\n",
        "\n",
        "    Rz = np.array([[np.cos(gamma), -np.sin(gamma), 0],\n",
        "                   [np.sin(gamma), np.cos(gamma), 0],\n",
        "                   [0, 0, 1]])\n",
        "\n",
        "    R = np.dot(np.dot(Rx, Ry), Rz)\n",
        "\n",
        "    X_rot = np.empty_like(X)\n",
        "    for channel in range(X.shape[-1]):\n",
        "        X_rot[:,:,:,channel] = affine_transform(X[:,:,:,channel], R, offset=0, order=3, mode='constant')\n",
        "    y_rot = affine_transform(y, R, offset=0, order=0, mode='constant')\n",
        "\n",
        "    del(Rx, Ry, Rz, R)\n",
        "    return X_rot, y_rot\n",
        "\n",
        "def brightness(X, y):\n",
        "    \"\"\"\n",
        "    Changing the brighness of a image using power-law gamma transformation.\n",
        "    Gain and gamma are chosen randomly for each image channel.\n",
        "\n",
        "    Gain chosen between [0.9 - 1.1]\n",
        "    Gamma chosen between [0.9 - 1.1]\n",
        "\n",
        "    new_im = gain * im^gamma\n",
        "    \"\"\"\n",
        "\n",
        "    X_new = np.zeros(X.shape)\n",
        "    for c in range(X.shape[-1]):\n",
        "        im = X[:,:,:,c]\n",
        "        gain, gamma = (1.2 - 0.8) * np.random.random_sample(2,) + 0.8\n",
        "        im_new = np.sign(im)*gain*(np.abs(im)**gamma)\n",
        "        X_new[:,:,:,c] = im_new\n",
        "\n",
        "    del(im, gain, gamma, im_new)\n",
        "    return X_new, y\n",
        "\n",
        "def elastic(X, y):\n",
        "    \"\"\"\n",
        "    Elastic deformation on a image and its target\n",
        "    \"\"\"\n",
        "    [Xel, yel] = elasticdeform.deform_random_grid([X, y], sigma=2, axis=[(0, 1, 2), (0, 1, 2)], order=[1, 0], mode='constant')\n",
        "\n",
        "    return Xel, yel\n",
        "\n",
        "def random_decisions(N):\n",
        "    \"\"\"\n",
        "    Generate N random decisions for augmentation\n",
        "    N should be equal to the batch size\n",
        "    \"\"\"\n",
        "\n",
        "    decisions = np.zeros((N, 4)) # 4 is number of aug techniques to combine (patch extraction excluded)\n",
        "    for n in range(N):\n",
        "        decisions[n] = np.random.randint(2, size=4)\n",
        "\n",
        "    return decisions\n",
        "\n",
        "def combine_aug(X, y, do):\n",
        "    \"\"\"\n",
        "    Combine randomly the different augmentation techniques written above\n",
        "    \"\"\"\n",
        "    Xnew, ynew = X, y\n",
        "\n",
        "    # make sure to use at least the 25% of original images\n",
        "    if np.random.random_sample()>0.75:\n",
        "        return Xnew, ynew\n",
        "\n",
        "    else:\n",
        "        if do[0] == 1:\n",
        "            Xnew, ynew = flip3D(Xnew, ynew)\n",
        "\n",
        "        if do[1] == 1:\n",
        "            Xnew, ynew = brightness(Xnew, ynew)\n",
        "\n",
        "        if do[2] == 1:\n",
        "            Xnew, ynew = rotation3D(Xnew, ynew)\n",
        "\n",
        "        if do[3] == 1:\n",
        "            Xnew, ynew = elastic(Xnew, ynew)\n",
        "\n",
        "        return Xnew, ynew\n",
        "\n",
        "def aug_batch(Xb, Yb):\n",
        "    \"\"\"\n",
        "    Generate a augmented image batch\n",
        "    \"\"\"\n",
        "    batch_size = len(Xb)\n",
        "    newXb, newYb = np.empty_like(Xb), np.empty_like(Yb)\n",
        "\n",
        "    decisions = random_decisions(batch_size)\n",
        "    inputs = [(X, y, do) for X, y, do in zip(Xb, Yb, decisions)]\n",
        "    pool = mp.Pool(processes=8)\n",
        "    multi_result = pool.starmap(combine_aug, inputs)\n",
        "    pool.close()\n",
        "\n",
        "    for i in range(len(Xb)):\n",
        "        newXb[i], newYb[i] = multi_result[i][0], multi_result[i][1]\n",
        "\n",
        "    del(decisions, inputs, multi_result, pool)\n",
        "    return newXb, newYb"
      ],
      "metadata": {
        "id": "25O0ebVE3MLK",
        "execution": {
          "iopub.status.busy": "2022-05-01T04:59:44.057734Z",
          "iopub.execute_input": "2022-05-01T04:59:44.05813Z",
          "iopub.status.idle": "2022-05-01T04:59:44.103379Z",
          "shell.execute_reply.started": "2022-05-01T04:59:44.058047Z",
          "shell.execute_reply": "2022-05-01T04:59:44.102162Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load image and data generator\n",
        "#generate train set and valid set\n",
        "\n",
        "def load_img(img_files):\n",
        "    ''' Load one image and its target form file\n",
        "    '''\n",
        "    N = len(img_files)\n",
        "    # target\n",
        "    y = nib.load(img_files[N-1]).get_fdata(dtype='float32', caching='unchanged')\n",
        "    y = y[40:200,34:226,8:136]\n",
        "    y[y==4]=3\n",
        "\n",
        "    X_norm = np.empty((240, 240, 155, 1))\n",
        "#     for channel in range(N-1):\n",
        "    X = nib.load(img_files[FUSECHANNEL]).get_fdata(dtype='float32', caching='unchanged')\n",
        "    brain = X[X!=0]\n",
        "    brain_norm = np.zeros_like(X)\n",
        "    # background at -100\n",
        "    norm = (brain - np.mean(brain))/np.std(brain)\n",
        "    brain_norm[X!=0] = norm\n",
        "    X_norm[:,:,:,0] = brain_norm\n",
        "\n",
        "    X_norm = X_norm[40:200,34:226,8:136,:]\n",
        "    del(X, brain, brain_norm)\n",
        "\n",
        "    return X_norm, y\n",
        "\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, batch_size=4, dim=(160,192,128), n_channels=1, n_classes=4, shuffle=True, augmentation=False, patch_size=128, n_patches=1):\n",
        "        'Initialization'\n",
        "        self.list_IDs = list_IDs\n",
        "        self.batch_size = batch_size\n",
        "        self.dim = dim\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.augmentation = augmentation\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = n_patches\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return len(self.list_IDs) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "        if self.augmentation == True:\n",
        "            X, y = self.__data_augmentation(X, y)\n",
        "\n",
        "        if index == self.__len__()-1:\n",
        "            self.on_epoch_end()\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size, *self.dim))\n",
        "\n",
        "        # Generate data\n",
        "        for i, IDs in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            X[i], y[i] = load_img(IDs)\n",
        "\n",
        "        if self.augmentation == True:\n",
        "            return X.astype('float32'), y\n",
        "        else:\n",
        "            return X.astype('float32'), to_categorical(y, self.n_classes)\n",
        "\n",
        "    def __data_augmentation(self, X, y):\n",
        "        'Apply augmentation'\n",
        "        X_aug, y_aug = patch_extraction(X, y, sizePatches=self.patch_size, Npatches=self.n_patches, apply_ratio=False)\n",
        "        X_aug, y_aug = aug_batch(X_aug, y_aug)\n",
        "\n",
        "        return X_aug, to_categorical(y_aug, self.n_classes)"
      ],
      "metadata": {
        "id": "4tjALFSnzl4S",
        "execution": {
          "iopub.status.busy": "2022-05-01T04:59:44.104735Z",
          "iopub.execute_input": "2022-05-01T04:59:44.104988Z",
          "iopub.status.idle": "2022-05-01T04:59:44.128267Z",
          "shell.execute_reply.started": "2022-05-01T04:59:44.104953Z",
          "shell.execute_reply": "2022-05-01T04:59:44.127552Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def double_conv(x, Nf, ks, norm=True):\n",
        "      for ss in range(2):\n",
        "          x = Conv3D(Nf, kernel_size=ks, strides=1, kernel_initializer='he_normal', padding='same')(x)\n",
        "\n",
        "          if (norm):\n",
        "              x = BatchNormalization()(x)\n",
        "          x = ReLU()(x)\n",
        "      return x\n",
        "\n",
        "def Generator():\n",
        "\n",
        "\n",
        "    def encoder_step(layer, Nf, ks, norm=True):\n",
        "        x = double_conv(layer, Nf/2, ks, norm)\n",
        "        x = Conv3D(Nf, kernel_size=ks, strides=2, kernel_initializer='he_normal', padding='same')(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def bottlenek(layer, Nf, ks):\n",
        "        x = Conv3D(Nf, kernel_size=ks, strides=2, kernel_initializer='he_normal', padding='same')(layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = ReLU()(x)\n",
        "\n",
        "        for i in range(4):\n",
        "            y = Conv3D(Nf, kernel_size=ks, strides=1, kernel_initializer='he_normal', padding='same')(x)\n",
        "            x = BatchNormalization()(y)\n",
        "            x = ReLU()(x)\n",
        "            x = Concatenate()([x, y])\n",
        "#             print(x.shape)\n",
        "        return x\n",
        "\n",
        "    def decoder_step(layer, layer_to_concatenate, Nf, ks):\n",
        "\n",
        "        x = Dropout(0.1)(layer)\n",
        "        x = Conv3DTranspose(Nf, kernel_size=ks, strides=2, padding='same', kernel_initializer='he_normal')(x)\n",
        "        x = Concatenate()([x, layer_to_concatenate])\n",
        "        x = double_conv(x, Nf, ks)\n",
        "        return x\n",
        "\n",
        "    layers_to_concatenate = []\n",
        "    inputs = Input((128,128,128,1), name='input_image')\n",
        "    Nfilter_start = 8\n",
        "    depth = 5\n",
        "    ks = 4\n",
        "    x = inputs\n",
        "\n",
        "    # encoder\n",
        "    for d in range(depth):\n",
        "        if d==0:\n",
        "          x = Conv3D(Nfilter_start, kernel_size=ks, strides=1, kernel_initializer='he_normal', padding='same')(x)\n",
        "          x = Conv3D(Nfilter_start, kernel_size=ks, strides=1, kernel_initializer='he_normal', padding='same')(x)\n",
        "        else:\n",
        "          x = encoder_step(x, Nfilter_start*np.power(2,d), ks)\n",
        "\n",
        "        layers_to_concatenate.append(x)\n",
        "\n",
        "    # bottlenek\n",
        "    x = bottlenek(x, Nfilter_start*np.power(2,depth-1), ks)\n",
        "\n",
        "    # decoder\n",
        "    for d in range(depth-2, -1, -1):\n",
        "        x = decoder_step(x, layers_to_concatenate.pop(), Nfilter_start*np.power(2,d), ks)\n",
        "\n",
        "    # classifier\n",
        "    last = Conv3DTranspose(4, kernel_size=ks, strides=2, padding='same', kernel_initializer='he_normal', activation='softmax', name='output_generator')(x)\n",
        "\n",
        "    del(x,layers_to_concatenate)\n",
        "    return Model(inputs=inputs, outputs=last, name='Generator')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4lDxXmP44Dfq",
        "execution": {
          "iopub.status.busy": "2022-05-01T04:59:44.259878Z",
          "iopub.execute_input": "2022-05-01T04:59:44.260606Z",
          "iopub.status.idle": "2022-05-01T04:59:44.28401Z",
          "shell.execute_reply.started": "2022-05-01T04:59:44.260569Z",
          "shell.execute_reply": "2022-05-01T04:59:44.283367Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calcul losses\n",
        "\n",
        "\n",
        "def diceLoss(y_true, y_pred, class_weights):\n",
        "    y_true = tf.convert_to_tensor(y_true, 'float32')\n",
        "    y_pred = tf.convert_to_tensor(y_pred, y_true.dtype)\n",
        "\n",
        "    num = tf.math.reduce_sum(tf.math.multiply(class_weights, tf.math.reduce_sum(tf.math.multiply(y_true, y_pred), axis=[0,1,2,3])))\n",
        "    den = tf.math.reduce_sum(tf.math.multiply(class_weights, tf.math.reduce_sum(tf.math.add(y_true, y_pred), axis=[0,1,2,3])))+1e-5\n",
        "    return 1-2*num/den\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_fake_output):\n",
        "    real_loss = tf.math.reduce_mean(tf.math.pow(tf.ones_like(disc_real_output) - disc_real_output, 2))\n",
        "    fake_loss = tf.math.reduce_mean(tf.math.pow(tf.zeros_like(disc_fake_output) - disc_fake_output, 2))\n",
        "\n",
        "    disc_loss = 0.5*(real_loss + fake_loss)\n",
        "\n",
        "    return disc_loss\n",
        "\n",
        "\n",
        "def generator_loss(target, gen_output, disc_fake_output, class_weights, alpha):\n",
        "\n",
        "    # generalized dice loss\n",
        "    dice_loss = diceLoss(target, gen_output, class_weights)\n",
        "\n",
        "\n",
        "    gen_loss = dice_loss\n",
        "    disc_loss=0\n",
        "\n",
        "\n",
        "\n",
        "    return gen_loss, dice_loss, disc_loss"
      ],
      "metadata": {
        "id": "EopAuIWScX8C",
        "execution": {
          "iopub.status.busy": "2022-05-01T04:59:44.285232Z",
          "iopub.execute_input": "2022-05-01T04:59:44.28554Z",
          "iopub.status.idle": "2022-05-01T04:59:44.297588Z",
          "shell.execute_reply.started": "2022-05-01T04:59:44.285471Z",
          "shell.execute_reply": "2022-05-01T04:59:44.296929Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# *  **WT = ET ∪ ED ∪ NCR/NET**\n",
        "# *  **TC = ET ∪ NCR/NET**\n",
        "# *  **ET = ET**\n",
        "\n",
        "\n",
        "\n",
        "1.   ED (edema -> classe 2)\n",
        "2.   ET (enhancing_tumor -> classe 3)\n",
        "3.   NCR/NET (non_enhancing_tumor -> classe 1)\n",
        "4.   BG (background -> classe 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ntkNQlCr-t_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#METRICS EVALUATING\n",
        "\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = y_true.flatten()\n",
        "    y_pred_f = y_pred.flatten()\n",
        "    intersection = np.sum(y_true_f * y_pred_f)\n",
        "    smooth = 1e-5\n",
        "\n",
        "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
        "\n",
        "def binary_dice3d(s,g):\n",
        "    #dice score of two 3D volumes\n",
        "    smooth = 1e-5\n",
        "    num=np.sum(np.multiply(s, g))\n",
        "    denom=s.sum() + g.sum() + smooth\n",
        "\n",
        "    return  2.0*num/denom\n",
        "\n",
        "def sensitivity (seg,ground):\n",
        "    #computs false negative rate\n",
        "    smooth = 1e-5\n",
        "    num=np.sum(np.multiply(ground, seg))\n",
        "    denom=np.sum(ground)+smooth\n",
        "\n",
        "    return  num/denom\n",
        "\n",
        "def specificity (seg,ground):\n",
        "    #computes false positive rate\n",
        "    smooth = 1e-5\n",
        "    num=np.sum(np.multiply(ground==0, seg ==0))\n",
        "    denom=np.sum(ground==0)+smooth\n",
        "\n",
        "    return  num/denom\n",
        "\n",
        "def border_map(binary_img,neigh):\n",
        "    \"\"\"\n",
        "    Creates the border for a 3D image\n",
        "    \"\"\"\n",
        "    binary_map = np.asarray(binary_img, dtype=np.uint8)\n",
        "    neigh = neigh\n",
        "    west = ndimage.shift(binary_map, [-1, 0,0], order=0)\n",
        "    east = ndimage.shift(binary_map, [1, 0,0], order=0)\n",
        "    north = ndimage.shift(binary_map, [0, 1,0], order=0)\n",
        "    south = ndimage.shift(binary_map, [0, -1,0], order=0)\n",
        "    top = ndimage.shift(binary_map, [0, 0, 1], order=0)\n",
        "    bottom = ndimage.shift(binary_map, [0, 0, -1], order=0)\n",
        "    cumulative = west + east + north + south + top + bottom\n",
        "    border = ((cumulative < 6) * binary_map) == 1\n",
        "    return border\n",
        "\n",
        "def border_distance(ref,seg):\n",
        "    \"\"\"\n",
        "    This functions determines the map of distance from the borders of the\n",
        "    segmentation and the reference and the border maps themselves\n",
        "    \"\"\"\n",
        "    neigh=8\n",
        "    border_ref = border_map(ref,neigh)\n",
        "    border_seg = border_map(seg,neigh)\n",
        "    oppose_ref = 1 - ref\n",
        "    oppose_seg = 1 - seg\n",
        "    # euclidean distance transform\n",
        "    distance_ref = ndimage.distance_transform_edt(oppose_ref)\n",
        "    distance_seg = ndimage.distance_transform_edt(oppose_seg)\n",
        "    distance_border_seg = border_ref * distance_seg\n",
        "    distance_border_ref = border_seg * distance_ref\n",
        "    return distance_border_ref, distance_border_seg#, border_ref, border_seg\n",
        "\n",
        "def Hausdorff_distance(ref,seg):\n",
        "    \"\"\"\n",
        "    This functions calculates the average symmetric distance and the\n",
        "    hausdorff distance between a segmentation and a reference image\n",
        "    :return: hausdorff distance and average symmetric distance\n",
        "    \"\"\"\n",
        "    ref_border_dist, seg_border_dist = border_distance(ref,seg)\n",
        "    hausdorff_distance = np.max([np.max(ref_border_dist), np.max(seg_border_dist)])\n",
        "    return hausdorff_distance\n",
        "\n",
        "def DSC_whole(pred, orig_label):\n",
        "    #computes dice for the whole tumor\n",
        "    return dice_coef(pred>0,orig_label>0)\n",
        "\n",
        "def DSC_en(pred, orig_label):\n",
        "    #computes dice for enhancing region\n",
        "    return dice_coef(pred==3,orig_label==3)\n",
        "\n",
        "def DSC_core(pred, orig_label):\n",
        "    #computes dice for core region\n",
        "    seg_=np.copy(pred)\n",
        "    ground_=np.copy(orig_label)\n",
        "    seg_[seg_==2]=0\n",
        "    ground_[ground_==2]=0\n",
        "    return dice_coef(seg_>0,ground_>0)\n",
        "\n",
        "def sensitivity_whole (seg,ground):\n",
        "    return sensitivity(seg>0,ground>0)\n",
        "\n",
        "def sensitivity_en (seg,ground):\n",
        "    return sensitivity(seg==3,ground==3)\n",
        "\n",
        "def sensitivity_core (seg,ground):\n",
        "    seg_=np.copy(seg)\n",
        "    ground_=np.copy(ground)\n",
        "    seg_[seg_==2]=0\n",
        "    ground_[ground_==2]=0\n",
        "    return sensitivity(seg_>0,ground_>0)\n",
        "\n",
        "def specificity_whole (seg,ground):\n",
        "    return specificity(seg>0,ground>0)\n",
        "\n",
        "def specificity_en (seg,ground):\n",
        "    return specificity(seg==3,ground==3)\n",
        "\n",
        "def specificity_core (seg,ground):\n",
        "    seg_=np.copy(seg)\n",
        "    ground_=np.copy(ground)\n",
        "    seg_[seg_==2]=0\n",
        "    ground_[ground_==2]=0\n",
        "    return specificity(seg_>0,ground_>0)\n",
        "\n",
        "def hausdorff_whole (seg,ground):\n",
        "    return Hausdorff_distance(seg==0,ground==0)\n",
        "\n",
        "def hausdorff_en (seg,ground):\n",
        "    return Hausdorff_distance(seg!=3,ground!=3)\n",
        "\n",
        "def hausdorff_core (seg,ground):\n",
        "    seg_=np.copy(seg)\n",
        "    ground_=np.copy(ground)\n",
        "    seg_[seg_==2]=0\n",
        "    ground_[ground_==2]=0\n",
        "    return Hausdorff_distance(seg_==0,ground_==0)"
      ],
      "metadata": {
        "id": "UpgY_tqqVgMj",
        "execution": {
          "iopub.status.busy": "2022-05-01T04:59:44.300412Z",
          "iopub.execute_input": "2022-05-01T04:59:44.300758Z",
          "iopub.status.idle": "2022-05-01T04:59:44.325843Z",
          "shell.execute_reply.started": "2022-05-01T04:59:44.30072Z",
          "shell.execute_reply": "2022-05-01T04:59:44.324998Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_weights = np.load(INPUT_PATH + '/class_weights.npy')\n",
        "\n",
        "# Models\n",
        "G = Generator()\n",
        "\n",
        "\n",
        "if os.path.exists(PATH + f'/{VERSION}_Generator.h5')==True :\n",
        "    G.load_weights(PATH + f'/{VERSION}_Generator.h5')\n",
        "\n",
        "\n",
        "# Optimizers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-3, beta_1=0.5)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(image, target, alpha):\n",
        "\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "\n",
        "      gen_output = G(image, training=True)\n",
        "\n",
        "      disc_fake_output=0\n",
        "      disc_loss=0\n",
        "      gen_loss, dice_loss, disc_loss_gen = generator_loss(target, gen_output, disc_fake_output, class_weights, alpha)\n",
        "      del gen_output, disc_fake_output,image\n",
        "  generator_gradients = gen_tape.gradient(gen_loss, G.trainable_variables)\n",
        "\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients, G.trainable_variables))\n",
        "\n",
        "\n",
        "  return gen_loss, dice_loss, disc_loss_gen\n",
        "\n",
        "@tf.function\n",
        "def test_step(image, target, alpha):\n",
        "    gen_output = G(image, training=False)\n",
        "\n",
        "    disc_fake_output=0\n",
        "    disc_loss=0\n",
        "    gen_loss, dice_loss, disc_loss_gen = generator_loss(target, gen_output, disc_fake_output, class_weights, alpha)\n",
        "    del gen_output,image\n",
        "    return gen_loss, dice_loss, disc_loss_gen\n",
        "\n",
        "def fit(train_gen, valid_gen, alpha, epochs, jump):\n",
        "\n",
        "    if os.path.exists(PATH)==False:\n",
        "        os.mkdir(PATH)\n",
        "\n",
        "    Nt = len(train_gen)\n",
        "    history = {'train': [], 'valid': []}\n",
        "\n",
        "    stats={'dice':[],'spec':[],'sen':[],'hau95':[]}\n",
        "    prev_loss = np.inf\n",
        "\n",
        "    epoch_ugan_loss = tf.keras.metrics.Mean()\n",
        "    epoch_dice_loss = tf.keras.metrics.Mean()\n",
        "    epoch_disc_loss = tf.keras.metrics.Mean()\n",
        "\n",
        "    epoch_v2v_loss_val = tf.keras.metrics.Mean()\n",
        "    epoch_dice_loss_val = tf.keras.metrics.Mean()\n",
        "    epoch_disc_loss_val = tf.keras.metrics.Mean()\n",
        "    ugan_min = 2\n",
        "\n",
        "    for e in range(epochs):\n",
        "        print('Epoch {}/{}'.format(e+1,epochs))\n",
        "        b = 0\n",
        "\n",
        "        x=(EXEC -1)*EPOCHS + e\n",
        "        lrg=(math.exp(-x))*0.001 + 0.00001\n",
        "        lrd=(math.exp(-x))*0.001 + 0.00002\n",
        "        K.set_value(generator_optimizer.learning_rate, lrg)\n",
        "\n",
        "\n",
        "        for Xb, yb in tqdm(train_gen):\n",
        "\n",
        "          b += 1\n",
        "          losses = train_step(Xb, yb, alpha)\n",
        "          epoch_ugan_loss.update_state(losses[0])\n",
        "          epoch_dice_loss.update_state(losses[1])\n",
        "          epoch_disc_loss.update_state(losses[2])\n",
        "\n",
        "          print('\\n Batch: {}/{} - loss: {:.4f} - dice_loss: {:.4f} - disc_loss: {:.4f}'\n",
        "                        .format(b, Nt, epoch_ugan_loss.result(), epoch_dice_loss.result(), 0))\n",
        "\n",
        "\n",
        "        del Xb,yb\n",
        "        history['train'].append([epoch_ugan_loss.result(), epoch_dice_loss.result(), 0])\n",
        "\n",
        "\n",
        "        for Xb, yb in tqdm(valid_gen):\n",
        "            losses_val = test_step(Xb, yb, alpha)\n",
        "            epoch_v2v_loss_val.update_state(losses_val[0])\n",
        "            epoch_dice_loss_val.update_state(losses_val[1])\n",
        "            epoch_disc_loss_val.update_state(losses_val[2])\n",
        "\n",
        "\n",
        "        print('\\n               loss_val: {:.4f} - dice_loss_val: {:.4f} - disc_loss_val: {:.4f}'\n",
        "                     .format(epoch_v2v_loss_val.result(), epoch_dice_loss_val.result(), 0))\n",
        "#         stdout.flush()\n",
        "        history['valid'].append([epoch_v2v_loss_val.result(), epoch_dice_loss_val.result(), 0])\n",
        "\n",
        "        # save pred image at epoch e\n",
        "        y_pred = G.predict(Xb)\n",
        "        y_true = np.argmax(yb, axis=-1)\n",
        "        y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "        canvas = np.zeros((128, 128*3))\n",
        "        idx = np.random.randint(len(Xb))\n",
        "\n",
        "        x = Xb[idx,:,:,64,0]\n",
        "        canvas[0:128, 0:128] = (x - np.min(x))/(np.max(x)-np.min(x)+1e-6)\n",
        "        canvas[0:128, 128:2*128] = y_true[idx,:,:,64]/3\n",
        "        canvas[0:128, 2*128:3*128] = y_pred[idx,:,:,64]/3\n",
        "\n",
        "        if jump:\n",
        "            n_e = e + (EXEC-1)*epochs\n",
        "        else:\n",
        "            n_e = e\n",
        "\n",
        "        fname = (PATH + f'/{VERSION}'+'_pred@epoch_{:03d}.png').format(n_e+1)\n",
        "        mpim.imsave(fname, canvas, cmap='gray')\n",
        "\n",
        "\n",
        "\n",
        "        try :\n",
        "\n",
        "                G.save_weights(PATH + f'/{VERSION}_Generator.h5')\n",
        "\n",
        "\n",
        "        except:\n",
        "            print('Weights not saved')\n",
        "\n",
        "\n",
        "        epoch_ugan_loss.reset_states()\n",
        "        epoch_dice_loss.reset_states()\n",
        "        epoch_disc_loss.reset_states()\n",
        "        epoch_v2v_loss_val.reset_states()\n",
        "        epoch_dice_loss_val.reset_states()\n",
        "        epoch_disc_loss_val.reset_states()\n",
        "\n",
        "\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "booFxn3o7Mcg",
        "execution": {
          "iopub.status.busy": "2022-05-01T04:59:44.327375Z",
          "iopub.execute_input": "2022-05-01T04:59:44.32794Z",
          "iopub.status.idle": "2022-05-01T04:59:47.465479Z",
          "shell.execute_reply.started": "2022-05-01T04:59:44.327902Z",
          "shell.execute_reply": "2022-05-01T04:59:47.464731Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T04:59:47.466815Z",
          "iopub.execute_input": "2022-05-01T04:59:47.467049Z",
          "iopub.status.idle": "2022-05-01T04:59:48.14147Z",
          "shell.execute_reply.started": "2022-05-01T04:59:47.467016Z",
          "shell.execute_reply": "2022-05-01T04:59:48.14007Z"
        },
        "trusted": true,
        "id": "GJ4Fh_fHOLDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "zip_file = tarfile.open(\"/kaggle/input/brats-2021-task1/BraTS2021_Training_Data.tar\")\n",
        "\n",
        "zip_file.extractall(DATA_PATH)\n",
        "zip_file.close()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T04:59:48.146171Z",
          "iopub.execute_input": "2022-05-01T04:59:48.146573Z",
          "iopub.status.idle": "2022-05-01T05:02:07.891418Z",
          "shell.execute_reply.started": "2022-05-01T04:59:48.14653Z",
          "shell.execute_reply": "2022-05-01T05:02:07.890666Z"
        },
        "trusted": true,
        "id": "n6cw321WOLDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1_list = sorted(glob.glob(DATA_PATH+'/*/*_t1.nii.gz'))\n",
        "t2_list = sorted(glob.glob(DATA_PATH+'/*/*_t2.nii.gz'))\n",
        "t1ce_list = sorted(glob.glob(DATA_PATH+'/*/*_t1ce.nii.gz'))\n",
        "flair_list = sorted(glob.glob(DATA_PATH+'/*/*_flair.nii.gz'))\n",
        "seg_list = sorted(glob.glob(DATA_PATH+'/*/*_seg.nii.gz'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T05:04:03.840206Z",
          "iopub.execute_input": "2022-05-01T05:04:03.840752Z",
          "iopub.status.idle": "2022-05-01T05:04:04.01863Z",
          "shell.execute_reply.started": "2022-05-01T05:04:03.840716Z",
          "shell.execute_reply": "2022-05-01T05:04:04.017849Z"
        },
        "trusted": true,
        "id": "MzakPnj2OLDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(t1_list),len(t2_list),len(t1ce_list),len(flair_list),len(seg_list))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T05:02:08.072811Z",
          "iopub.execute_input": "2022-05-01T05:02:08.073056Z",
          "iopub.status.idle": "2022-05-01T05:02:08.078898Z",
          "shell.execute_reply.started": "2022-05-01T05:02:08.073022Z",
          "shell.execute_reply": "2022-05-01T05:02:08.078275Z"
        },
        "trusted": true,
        "id": "XEuYL0j1OLDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#*******************MAIN**********************************************************\n",
        "#*********************************************************************************\n",
        "#EXTRACTION OF DATA , T1 T2 T1C TFLAIR GROUNDTRUE\n",
        "BATCH_SIZE = 4\n",
        "ALPHA = 5\n",
        "EPOCHS = 1\n",
        "NB_CLASSES = 4\n",
        "N_PATCHES = 1\n",
        "\n",
        "JUMP = False\n",
        "pred_img = (PATH + f'/{VERSION}'+'_pred@epoch_{:03d}.png').format((EXEC-1)*EPOCHS) #0,8,16\n",
        "if os.path.exists(pred_img)==True:\n",
        "    JUMP = True\n",
        "\n",
        "# create the training and validation sets or load it from inputs\n",
        "#Added here\n",
        "Nim = len(t1_list)\n",
        "\n",
        "idx = np.arange(Nim)\n",
        "sets = {'train': [], 'valid': []}\n",
        "with open(INPUT_PATH + \"/split.json\", \"r\") as read_file:\n",
        "\n",
        "    sets = json.load(read_file)\n",
        "\n",
        "# for i in range(len(t1_list)):\n",
        "#     sets['train'].append([t1_list[i], t2_list[i], t1ce_list[i], flair_list[i], seg_list[i]])\n",
        "\n",
        "\n",
        "del(t1_list,t2_list,t1ce_list,flair_list,seg_list,idx,Nim)\n",
        "\n",
        "train_gen = DataGenerator(sets['train'], batch_size=BATCH_SIZE, n_classes=NB_CLASSES, n_patches=N_PATCHES, augmentation=True)\n",
        "valid_gen = DataGenerator(sets['valid'], batch_size=BATCH_SIZE, n_classes=NB_CLASSES, n_patches=N_PATCHES, augmentation=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "B2Bn5bF35LTG",
        "execution": {
          "iopub.status.busy": "2022-05-01T05:03:55.301837Z",
          "iopub.execute_input": "2022-05-01T05:03:55.302112Z",
          "iopub.status.idle": "2022-05-01T05:03:55.327778Z",
          "shell.execute_reply.started": "2022-05-01T05:03:55.302069Z",
          "shell.execute_reply": "2022-05-01T05:03:55.326796Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sets['train']))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T05:04:39.977243Z",
          "iopub.execute_input": "2022-05-01T05:04:39.977838Z",
          "iopub.status.idle": "2022-05-01T05:04:39.982577Z",
          "shell.execute_reply.started": "2022-05-01T05:04:39.9778Z",
          "shell.execute_reply": "2022-05-01T05:04:39.981661Z"
        },
        "trusted": true,
        "id": "Rp0TORcxOLDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VJmnJDcoOLDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap --no-stderr\n",
        "h = fit(train_gen, valid_gen, ALPHA, EPOCHS, JUMP)"
      ],
      "metadata": {
        "id": "jQhIFAKdn-xt",
        "execution": {
          "iopub.status.busy": "2022-05-01T05:02:08.433721Z",
          "iopub.status.idle": "2022-05-01T05:02:08.434617Z",
          "shell.execute_reply.started": "2022-05-01T05:02:08.434365Z",
          "shell.execute_reply": "2022-05-01T05:02:08.434392Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(PATH + f'/{VERSION}_fit_output_{EXEC}.txt', 'w') as f:\n",
        "    f.write(cap.stdout)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T05:02:08.435862Z",
          "iopub.status.idle": "2022-05-01T05:02:08.436287Z",
          "shell.execute_reply.started": "2022-05-01T05:02:08.436043Z",
          "shell.execute_reply": "2022-05-01T05:02:08.436066Z"
        },
        "trusted": true,
        "id": "AofaVT29OLDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap_h --no-stderr\n",
        "print(h)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T05:02:08.43754Z",
          "iopub.status.idle": "2022-05-01T05:02:08.43818Z",
          "shell.execute_reply.started": "2022-05-01T05:02:08.437921Z",
          "shell.execute_reply": "2022-05-01T05:02:08.437945Z"
        },
        "trusted": true,
        "id": "PDQ9ti74OLDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(PATH + f'/{VERSION}_h_{EXEC}.txt', 'w') as f:\n",
        "    f.write(cap_h.stdout)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T05:02:08.439672Z",
          "iopub.status.idle": "2022-05-01T05:02:08.440072Z",
          "shell.execute_reply.started": "2022-05-01T05:02:08.439853Z",
          "shell.execute_reply": "2022-05-01T05:02:08.439875Z"
        },
        "trusted": true,
        "id": "kBAG-n4uOLDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#last loop for metrics\n",
        "from sklearn.metrics import precision_score\n",
        "#Importing tqdm function of tqdm module\n",
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "\n",
        "G = Generator()\n",
        "\n",
        "\n",
        "G.load_weights(PATH + f'/{VERSION}_Generator.h5')\n",
        "\n",
        "\n",
        "stats={'dice':[],'spec':[],'sen':[],'hau95':[]}\n",
        "i=0\n",
        "\n",
        "for Xb,yb in tqdm(valid_gen):\n",
        "  for k in range(len(Xb)):\n",
        "    segG = G.predict(Xb)[k,:]\n",
        "    segTr = yb[k,:]\n",
        "    case = 0\n",
        "    i=i+1\n",
        "    segG_all = np.argmax(segG, axis=-1)\n",
        "    segTr_all = np.argmax(segTr, axis=-1)\n",
        "\n",
        "    dscET=DSC_en(segG_all,segTr_all)\n",
        "    dscWT=DSC_whole(segG_all,segTr_all)\n",
        "    dscTC=DSC_core(segG_all,segTr_all)\n",
        "\n",
        "    specET= specificity_en(segG_all,segTr_all)\n",
        "    specWT= specificity_whole(segG_all,segTr_all)\n",
        "    specTC= specificity_core(segG_all,segTr_all)\n",
        "\n",
        "    senET= sensitivity_en(segG_all,segTr_all)\n",
        "    senWT= sensitivity_whole(segG_all,segTr_all)\n",
        "    senTC= sensitivity_core(segG_all,segTr_all)\n",
        "\n",
        "    hauET= hausdorff_en(segG_all,segTr_all)\n",
        "    hauWT= hausdorff_whole(segG_all,segTr_all)\n",
        "    hauTC= hausdorff_core(segG_all,segTr_all)\n",
        "\n",
        "    stats['dice'].append({'caseID':int(case),'ET':dscET,'WT':dscWT,'TC':dscTC})\n",
        "    stats['spec'].append({'caseID':int(case),'ET':specET,'WT':specWT,'TC':specTC})\n",
        "    stats['sen'].append({'caseID':int(case),'ET':senET,'WT':senWT,'TC':senTC})\n",
        "    stats['hau95'].append({'caseID':int(case),'ET':hauET,'WT':hauWT,'TC':hauTC})\n",
        "\n",
        "    del(segG,segG_all,segTr,segTr_all)\n",
        "\n",
        "with open(PATH + f'/{VERSION}_stats_{EXEC}.json', 'w') as f:\n",
        "          json.dump(stats, f, indent=2)\n",
        "          print(\"stats saved.\")\n"
      ],
      "metadata": {
        "id": "gjix6l1YOLDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap_metrics --no-stderr\n",
        "dicewt = []\n",
        "diceet = []\n",
        "dicetc = []\n",
        "senwt = []\n",
        "senet = []\n",
        "sentc = []\n",
        "specwt = []\n",
        "specet = []\n",
        "spectc = []\n",
        "hauwt =[]\n",
        "hauet = []\n",
        "hautc = []\n",
        "\n",
        "# print (stats['dice'][0]['WT'])\n",
        "for k in range(len(valid_gen)*BATCH_SIZE):\n",
        "  dicewt.append(stats['dice'][k]['WT'])\n",
        "  diceet.append(stats['dice'][k]['ET'])\n",
        "  dicetc.append(stats['dice'][k]['TC'])\n",
        "  senwt.append(stats['sen'][k]['WT'])\n",
        "  senet.append(stats['sen'][k]['ET'])\n",
        "  sentc.append( stats['sen'][k]['TC'])\n",
        "  specwt.append(stats['spec'][k]['WT'])\n",
        "  specet.append(stats['spec'][k]['ET'])\n",
        "  spectc.append(stats['spec'][k]['TC'])\n",
        "  hauwt.append(stats['hau95'][k]['WT'])\n",
        "  hauet.append(stats['hau95'][k]['ET'])\n",
        "  hautc.append( stats['hau95'][k]['TC'])\n",
        "\n",
        "print('Dice WT: ',np.mean(dicewt))\n",
        "print('Dice ET: ',np.mean(diceet))\n",
        "print('Dice TC: ',np.mean(dicetc))\n",
        "\n",
        "print('Sen WT: ',np.mean(senwt))\n",
        "print('Sen ET: ',np.mean(senet))\n",
        "print('Sen TC: ',np.mean(sentc))\n",
        "\n",
        "print('Spec WT: ',np.mean(specwt))\n",
        "print('Spec ET: ',np.mean(specet))\n",
        "print('Spec TC: ',np.mean(spectc))\n",
        "\n",
        "print('Hau95 WT: ',np.mean(hauwt))\n",
        "print('Hau95 ET: ',np.mean(hauet))\n",
        "print('Hau95 TC: ',np.mean(hautc))\n"
      ],
      "metadata": {
        "id": "_TwuKzmGOLDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(PATH + f'/{VERSION}_metrics_{EXEC}.txt', 'w') as f:\n",
        "    f.write(cap_metrics.stdout)"
      ],
      "metadata": {
        "id": "qtTtgYJlOLDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zip -r './results.zip' {PATH+'/*'}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T05:02:08.447294Z",
          "iopub.status.idle": "2022-05-01T05:02:08.447756Z",
          "shell.execute_reply.started": "2022-05-01T05:02:08.447488Z",
          "shell.execute_reply": "2022-05-01T05:02:08.447521Z"
        },
        "trusted": true,
        "id": "pNxTMJHnOLDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "FileLink(r'./results.zip')\n",
        "# then in colab : !wget \"https://....kaggle.net/...../results.zip\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-01T05:02:08.449126Z",
          "iopub.status.idle": "2022-05-01T05:02:08.449696Z",
          "shell.execute_reply.started": "2022-05-01T05:02:08.449456Z",
          "shell.execute_reply": "2022-05-01T05:02:08.449479Z"
        },
        "trusted": true,
        "id": "TAhUN2zyOLDi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}